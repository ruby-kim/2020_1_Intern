{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Week 9] NN(2): MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tQpNJJdE8ho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T3oNYEvSWMc",
        "colab_type": "text"
      },
      "source": [
        "# MLP(Multi Layer Perceptron)\n",
        "### 이해하기  \n",
        "  * 기존의 perceptron으로는 **XOR Gate**를 해결할 수 없었음\n",
        "  * XOR 게이트는 AND, OR, NOT gate의 조합으로 만들 수 있음 + perceptron으로는 AND와 OR gate를 만들 수 있음<br>= 여러 개의 perceptron으로 XOR 생성 가능!\n",
        "<br><br>\n",
        "\n",
        "### 구조  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;![image](https://user-images.githubusercontent.com/38516906/75954862-fe069a00-5ef7-11ea-8420-49f81f4115c0.png)\n",
        "  * [참고] 용어 설명\n",
        "    * DNN(Deep Neural Network): Hidden Layer가 3층 이상일 시\n",
        "      * 딥러닝(Deep Learning)은 Hidden Layer가 3개 이상인 인공 신경망을 의미함\n",
        "  * [참고] XOR 게이트 세부 설명  \n",
        "    ![image](https://user-images.githubusercontent.com/38516906/75955237-c51af500-5ef8-11ea-8208-47128bf7759d.png)\n",
        "  * [참고] layer의 개수에 따라 결정할 수 있는 영역  \n",
        "   ![image](https://user-images.githubusercontent.com/38516906/75955337-00b5bf00-5ef9-11ea-9959-c3bab8412890.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT6PGawNXMnx",
        "colab_type": "code",
        "outputId": "ff587b55-b3e6-49f0-9c99-ba72c0fb3ce4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# data 생성\n",
        "x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype = np.float32)\n",
        "y_data = np.array([[0],   [1],   [1],   [0]], dtype = np.float32)\n",
        "\n",
        "\n",
        "\n",
        "# placeholder 생성. 간단하게 값들을 담는 상자라고 생각하면 됨\n",
        "X = tf.placeholder(tf.float32)\n",
        "Y = tf.placeholder(tf.float32)\n",
        "\n",
        "\n",
        "\n",
        "# weight와 bias 생성\n",
        "W1 = tf.Variable(tf.random_normal([2, 2]), name = \"weight1\") # 행2, 열2인 랜덤 값 생성 / 입력 2개, 출력 2개\n",
        "b1 = tf.Variable(tf.random_normal([2]), name = \"bias1\") # w1의 출력이 2개이므로 2로 맞춰줌\n",
        "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
        "\n",
        "W2 = tf.Variable(tf.random_normal([2, 1]), name = \"weight2\") # 앞에서 출력이 2였기 때문에 2로 맞춰줌\n",
        "b2 = tf.Variable(tf.random_normal([1]), name = \"bias2\") # 최종결과는 1개의 출력값\n",
        "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
        "\n",
        "\n",
        "\n",
        "# cost function / minimize cost\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1-hypothesis))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
        "\n",
        "\n",
        "\n",
        "# predict / accuracy\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32) # cast: 조건따라 0,1 반환\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32)) # equal: x,y를 비교하여 boolean값 반환\n",
        "\n",
        "\n",
        "\n",
        "# start\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for step in range(10001):\n",
        "    sess.run(train, feed_dict = {X: x_data, Y: y_data})\n",
        "    if step%1000 == 0:\n",
        "      print(step, sess.run(cost, feed_dict = {X:x_data, Y:y_data}), sess.run([W1, W2]))\n",
        "  h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X: x_data, Y: y_data})\n",
        "  print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.7431668 [array([[ 0.03203906,  0.13442741],\n",
            "       [ 0.8116213 , -0.7063473 ]], dtype=float32), array([[-0.17556576],\n",
            "       [ 0.18559927]], dtype=float32)]\n",
            "1000 0.6929332 [array([[-0.06907163,  0.24811243],\n",
            "       [ 0.78561455, -0.727968  ]], dtype=float32), array([[-0.37243015],\n",
            "       [ 0.20722745]], dtype=float32)]\n",
            "2000 0.6916109 [array([[-0.13653436,  0.488732  ],\n",
            "       [ 0.7752193 , -0.8092532 ]], dtype=float32), array([[-0.32476747],\n",
            "       [ 0.42201453]], dtype=float32)]\n",
            "3000 0.6805143 [array([[-0.1650547 ,  1.1335862 ],\n",
            "       [ 0.76379526, -1.2344755 ]], dtype=float32), array([[-0.2735066],\n",
            "       [ 1.0193864]], dtype=float32)]\n",
            "4000 0.6023091 [array([[-0.11118392,  2.568695  ],\n",
            "       [ 0.70205426, -2.8343139 ]], dtype=float32), array([[-0.00531138],\n",
            "       [ 2.5519109 ]], dtype=float32)]\n",
            "5000 0.5014194 [array([[-0.7277731,  3.9033897],\n",
            "       [ 1.0780874, -4.3570824]], dtype=float32), array([[1.064168],\n",
            "       [4.057296]], dtype=float32)]\n",
            "6000 0.17787896 [array([[-3.4065104,  5.009461 ],\n",
            "       [ 3.063252 , -5.2426677]], dtype=float32), array([[4.599841 ],\n",
            "       [5.5582175]], dtype=float32)]\n",
            "7000 0.071612746 [array([[-4.547301 ,  5.5890656],\n",
            "       [ 4.2063704, -5.709346 ]], dtype=float32), array([[6.6215134],\n",
            "       [6.8325305]], dtype=float32)]\n",
            "8000 0.04289072 [array([[-5.0610476,  5.8848443],\n",
            "       [ 4.7252574, -5.990579 ]], dtype=float32), array([[7.6606803],\n",
            "       [7.6799045]], dtype=float32)]\n",
            "9000 0.030251024 [array([[-5.373637,  6.075915],\n",
            "       [ 5.041886, -6.184776]], dtype=float32), array([[8.3442  ],\n",
            "       [8.291258]], dtype=float32)]\n",
            "10000 0.023244604 [array([[-5.5931168,  6.2154436],\n",
            "       [ 5.2646346, -6.330794 ]], dtype=float32), array([[8.851266],\n",
            "       [8.764892]], dtype=float32)]\n",
            "\n",
            "Hypothesis:  [[0.02699867]\n",
            " [0.97793865]\n",
            " [0.9804491 ]\n",
            " [0.02328044]] \n",
            "Correct:  [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "Accuracy:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3upi8iWEKOnQ",
        "colab_type": "text"
      },
      "source": [
        "### 성능 향상\n",
        "원하는 결과인, Accuracy가 1인 모델을 만들었다.  \n",
        "하지만 정확도를 더 올리고 싶다면?\n",
        "1. 첫번째 layer의 결과값을 2보다 큰 수로 늘린다(Wide Neural Network)\n",
        "   * w1의 [2,2] -> [2,10]\n",
        "   * b1의 [2] -> [10]\n",
        "   * w2의 [2,1] -> [10,1]\n",
        "   * b2의 [1] -> [1]  \n",
        "   \n",
        "   그러면 hypothesis의 작은 값은 더 작게, 큰 값은 더 크게 되는 현상을 볼 수 있다.\n",
        "\n",
        "2. layer를 깊게 쌓는다(Deep Neural Network)\n",
        "   * 원하는대로 layer 생성: w3 & b3 / w4 & b4 , ..."
      ]
    }
  ]
}