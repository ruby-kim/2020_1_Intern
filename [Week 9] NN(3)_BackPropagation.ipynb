{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Week 9] NN(3): BackPropagation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T3oNYEvSWMc",
        "colab_type": "text"
      },
      "source": [
        "# Backpropagation\n",
        "### 들어가기\n",
        "* 우리는 NN(2): MLP를 통해 XOR를 예측했었다.\n",
        "* 그런데, 우리는 정확한 W나 b에 값을 구하지는 않았다.(정확히는 못했다)\n",
        "* 그럼 이런 상황에서 어떻게 W나 b를 구할까?\n",
        "\n",
        "<br><br>\n",
        "\n",
        "### W와 b 구하기\n",
        "* 바로 Gradient의 개념을 이용하면 구할 수 있다.  \n",
        "  ![image](https://user-images.githubusercontent.com/38516906/76195366-78eaf000-622b-11ea-94e2-95807d1b9301.png)\n",
        "* 우리는 지금까지 cost함수 == 결과값 Y로 정의했었다.\n",
        "* 그리고 위의 그림과 같은 그래프형태가 나타날 때, w가 어떤 초기값으로 시작하는지에 상관없이 그 점에서 **기울기**를 이용하여 global minimum을 찾을 수 있었다.\n",
        "* 즉, 이것을 구현하기 위해서는 특정 위치의 미분값을 필요로 한다.\n",
        "* 하지만 이것이 NN(Neural Network)로 가면서 복잡해지기 시작했다.\n",
        "  <br>  \n",
        "  ![image](https://user-images.githubusercontent.com/38516906/76195783-67561800-622c-11ea-8575-dcf327101b27.png)\n",
        "* 위 그림에는 노드들이 한두개가 아니고, 각 노드에도 우리가 사용했던 sigmoid 함수를 사용하는 등의 상황이 있었기 때문이다.\n",
        "* 여기서 미분값을 구한다 = 입력값 X가 결과값 Y에 끼치는 영향을 알아야 한다는 것\n",
        "* 그런데 이러한 상황에서는 단순히 입력값 X 뿐만 아니라, **각각의 노드들에 대해서도 구해야** 각 노드들에 대한 w와 b를 구할 수 있게 된다.  \n",
        "(그래서 1969년 Minsky 교수님께서 아무도 이것을 구할 수 없다고 하실만큼 복잡한 과정이 되어버렸다)\n",
        "\n",
        "<br><br>\n",
        "\n",
        "### Backpropagation\n",
        "* 하지만 인간들은 진화를 거듭해오고, 결국 이 문제를 **Backpropagation**으로 해결하게 되었다..  \n",
        "(과연 우리는 이 험난하고 똑똑한 사회에서 취직할 수 있을지 짤막한 현타가 느껴진다)  \n",
        "  ![image](https://user-images.githubusercontent.com/38516906/76196028-d469ad80-622c-11ea-830c-0beaf5f772a0.png)\n",
        "* 우리가 forward로 진행하여 얻은 결과값과 실제값을 비교하여 얻은 error,  \n",
        "  즉 **cost**를 통해 다시 뒤로 돌아가는 과정(backward)을 통해  \n",
        "  어떻게 무엇을 설정해야 하는지 알아내겠다는 개념이다.\n",
        "* 먼저 이해를 위해 값을 임의로 정해보도록 하자.\n",
        "  1. forward(w = -2, x = 5, b = 3) -> 학습 데이터 가져오는 과정\n",
        "  2. backward ==> 여기서 실제 미분값을 구해보도록 하자.  \n",
        "  ![image](https://user-images.githubusercontent.com/38516906/76196311-607bd500-622d-11ea-9964-2cc525a496a1.png)  \n",
        "  위 그림을 보면\n",
        "  * g = wx\n",
        "    * dg/dx = w\n",
        "    * dg/dw = x\n",
        "  * 즉 f = wx + b = g + b\n",
        "    * df/dg = 1\n",
        "    * df/db = 1 \n",
        "  * 그럼 w와 x가 f에 미치는 영향을 구할 수 있다.\n",
        "    * df/dw = df/dg * dg/dw = 1 * 5 = 5\n",
        "    * df/dx = df/dg * dg/dx = 1 * (-2) = -2"
      ]
    }
  ]
}